{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f76b8ffa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this example, you'l explore how to evaluate LLM's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aa5c0d",
   "metadata": {},
   "source": [
    "### Note Huggingface\n",
    "\n",
    "For some models (if not most) you need to install the huggingface cli tool (terminal) and authenticate it with your huggingface account. Please do not sign up with privacy (student) sensitive information.\n",
    "\n",
    "https://huggingface.co/docs/huggingface_hub/main/en/guides/cli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b4b9c5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Setup <a id=\"setup\"></a>\n",
    "\n",
    "Start by importing all the necessary packages. Fix the errors by using pip install!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb969201",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All warnings suppressed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from datasets import load_dataset\n",
    "from typing import List, Optional, Dict, Any\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"All warnings suppressed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937eb240",
   "metadata": {},
   "source": [
    "The following cell defines functions to make everything easier. Just run it for now and don't worry about understanding it all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8ff86f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HUGGING FACE UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def validate_token():\n",
    "    \"\"\"\n",
    "    Validate the current Hugging Face token and display user information.\n",
    "    \n",
    "    This function should be called after setting up your token to ensure\n",
    "    it was configured correctly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to get token from environment variable first\n",
    "        token = None\n",
    "        for env_var in ['HUGGINGFACE_HUB_TOKEN', 'HF_TOKEN']:\n",
    "            token = os.environ.get(env_var)\n",
    "            if token:\n",
    "                break\n",
    "        \n",
    "        if token:\n",
    "            # Set the token for the API\n",
    "            api = HfApi(token=token)\n",
    "        else:\n",
    "            # Try without explicit token (maybe already logged in)\n",
    "            api = HfApi()\n",
    "            \n",
    "        user_info = api.whoami()\n",
    "        print(f\"‚úÖ Token validated successfully!\")\n",
    "        print(f\"   Logged in as: {user_info['name']}\")\n",
    "        print(f\"   Token type: {user_info.get('type', 'unknown')}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Token validation failed.\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        print(\"\\nüí° Please check that:\")\n",
    "        print(\"   1. Your token is correctly set\")\n",
    "        print(\"   2. Your token has the necessary permissions\") \n",
    "        print(\"   3. You have access to the Deepseek models\")\n",
    "        print(\"   4. Try running: huggingface-cli login\")\n",
    "        return False\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL SERVICE CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class ServeLLM:\n",
    "    \"\"\"\n",
    "    A service class for loading and running language models with proper memory management.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, device: str = \"auto\"):\n",
    "        \"\"\"\n",
    "        Initialize the ServeLLM instance.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name/path of the model to load\n",
    "            device (str): Device to load model on ('auto', 'cuda', 'cpu')\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = self._determine_device(device)\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "    \n",
    "    def _determine_device(self, device: str) -> str:\n",
    "        \"\"\"Determine the best device to use.\"\"\"\n",
    "        if device == \"auto\":\n",
    "            # Check for CUDA (NVIDIA) or ROCm (AMD) availability\n",
    "            if torch.cuda.is_available():\n",
    "                return \"cuda\"\n",
    "            # Check for Apple metal\n",
    "            elif torch.backends.mps.is_available():\n",
    "                return \"mps\"\n",
    "            # ROCm also uses torch.cuda API\n",
    "            return \"cpu\"\n",
    "        return device\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the tokenizer and model.\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading {self.model_name}...\")\n",
    "            \n",
    "            # Load tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                trust_remote_code=True,\n",
    "              #  local_files_only=True\n",
    "            )\n",
    "            \n",
    "            # Add padding token if not present\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            # Load model with appropriate settings\n",
    "            model_kwargs = {\n",
    "                \"trust_remote_code\": True,\n",
    "                \"dtype\": torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "               #\"local_files_only\": True\n",
    "            }\n",
    "            if self.device == \"mps\":\n",
    "                model_kwargs[\"low_cpu_mem_usage\"] = True\n",
    "            if self.device == \"cuda\":\n",
    "                model_kwargs[\"device_map\"] = \"auto\"\n",
    "            \n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                **model_kwargs\n",
    "            )\n",
    "            \n",
    "            if [\"cpu\", \"mps\"]:\n",
    "                self.model = self.model.to(self.device)\n",
    "                \n",
    "            print(f\"‚úÖ Model loaded successfully on {self.device}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_response(\n",
    "        self, \n",
    "        prompt: str, \n",
    "        max_tokens: int = 512,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.9,\n",
    "        do_sample: bool = True, \n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response to the given prompt.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): Input prompt\n",
    "            max_tokens (int): Maximum tokens to generate\n",
    "            temperature (float): Sampling temperature\n",
    "            top_p (float): Top-p sampling parameter\n",
    "            do_sample (bool): Whether to use sampling\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated response\n",
    "        \"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n",
    "        \n",
    "        try:\n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(\n",
    "                prompt, \n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=2048\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Generate\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    attention_mask=inputs.attention_mask,\n",
    "                    max_new_tokens=max_tokens,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    do_sample=do_sample,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                )\n",
    "            \n",
    "            # Decode response (exclude input tokens)\n",
    "            response_tokens = outputs[0][inputs.input_ids.shape[1]:]\n",
    "            response = self.tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "            \n",
    "            return response.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating response: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up model and free GPU memory.\"\"\"\n",
    "        if self.model is not None:\n",
    "            del self.model\n",
    "            self.model = None\n",
    "        \n",
    "        if self.tokenizer is not None:\n",
    "            del self.tokenizer\n",
    "            self.tokenizer = None\n",
    "        \n",
    "        # Clear GPU cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        if torch.mps.is_available():\n",
    "            torch.mps.empty_cache() \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "        print(\"üßπ Model cleaned up and memory freed\")\n",
    "    \n",
    "    def __enter__(self):\n",
    "        \"\"\"Context manager entry.\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Context manager exit with automatic cleanup.\"\"\"\n",
    "        self.cleanup()\n",
    "    \n",
    "    @staticmethod\n",
    "    def cleanup_all():\n",
    "        \"\"\"Static method to clean up GPU memory.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        if torch.mps.is_available():\n",
    "            torch.mps.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\"üßπ All GPU memory cleaned up\")\n",
    "\n",
    "# =============================================================================\n",
    "# DISPLAY UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def display_section_header(title: str, level: int = 1):\n",
    "    \"\"\"\n",
    "    Display a section header with appropriate formatting.\n",
    "    \n",
    "    Args:\n",
    "        title (str): Section title\n",
    "        level (int): Header level (1, 2, or 3)\n",
    "    \"\"\"\n",
    "    if level == 1:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"{title.upper()}\")\n",
    "        print('='*60)\n",
    "    elif level == 2:\n",
    "        print(f\"\\n{'-'*40}\")\n",
    "        print(f\"{title}\")\n",
    "        print('-'*40)\n",
    "    else:\n",
    "        print(f\"\\n{title}\")\n",
    "        print('¬∑'*len(title))\n",
    "\n",
    "def display_warning(message: str):\n",
    "    \"\"\"Display a warning message in a prominent way.\"\"\"\n",
    "    print(\"‚ö†Ô∏è  WARNING:\", message)\n",
    "\n",
    "def display_success(message: str):\n",
    "    \"\"\"Display a success message.\"\"\"\n",
    "    print(\"‚úÖ\", message)\n",
    "\n",
    "def display_info(message: str):\n",
    "    \"\"\"Display an info message.\"\"\"\n",
    "    print(\"‚ÑπÔ∏è\", message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31d7711",
   "metadata": {},
   "source": [
    "Qwen3 technical report \n",
    "https://arxiv.org/pdf/2505.09388\n",
    "\n",
    "If you can run 7b models try the new Olmo models! They claim to be very open and ethical.\n",
    "https://huggingface.co/allenai/collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2644855",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Model definitions\n",
    "# Feel free to add more models or test your own.\n",
    "small_models = [\"qwen/qwen3-0.6b-base\",\n",
    "                \"qwen/qwen3-0.6b\", \n",
    "                #\"qwen/qwen3-4b\"\n",
    "                ]\n",
    "#https://huggingface.co/meta-llama/Llama-Guard-3-8B\n",
    "llama_guard = \"meta-llama/Llama-Guard-3-8B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c47238",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Example Prompts <a id=\"exampleprompts\"></a>\n",
    "\n",
    "Here are a few selected problems of varying complexity to test different aspects of mathematical reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a15bdc16",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prompts defined:\n",
      "1. What is the area of a rectangle with a length of 8 units and a width of 5 units?\n",
      "2. Solve: 2x + 3 = 7\n",
      "3. What is the derivative of sin(x)?\n"
     ]
    }
   ],
   "source": [
    "# Test prompts for model comparison\n",
    "TEST_PROMPTS = [\n",
    "    \"What is the area of a rectangle with a length of 8 units and a width of 5 units?\",\n",
    "    \"Solve: 2x + 3 = 7\",\n",
    "    \"What is the derivative of sin(x)?\"\n",
    "]\n",
    "\n",
    "# Expected key information in correct answers\n",
    "EXPECTED_KEYWORDS = [\n",
    "    \"40\",      # 8 * 5 = 40\n",
    "    \"x = 2\",   # 2x + 3 = 7 ‚Üí 2x = 4 ‚Üí x = 2\n",
    "    \"cos(x)\"   # derivative of sin(x) is cos(x)\n",
    "]\n",
    "\n",
    "print(\"Test prompts defined:\")\n",
    "for i, prompt in enumerate(TEST_PROMPTS):\n",
    "    print(f\"{i+1}. {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a9b76b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Processing Function <a id=\"processing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0673963",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 1: Implement a function to get responses from the model\n",
    "\n",
    "The `ServeLLM` class is a wrapper we've created for you in `utils.py` to simplify model loading and inference. It handles GPU memory management, model initialization, and provides clean methods like `generate_response()`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dad14f",
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def process_prompts(model_name, prompts):\n",
    "    \"\"\"\n",
    "    Process a list of prompts with a given model and return responses.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    # You can change device=\"auto\" to cuda, cpu or mps to manually select where to do the compute\n",
    "    with ServeLLM(model_name, device=\"auto\") as llm:\n",
    "        for i, prompt in enumerate(prompts):\n",
    "\n",
    "            response = llm.generate_response(prompt)\n",
    "            print(f\"Processed prompt {i} of {len(prompts)}\")\n",
    "            results.append(response)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1e0e4e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now you can evaluate all models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61f23a0e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "PROCESSING qwen/qwen3-0.6b-base\n",
      "==================================================\n",
      "Loading qwen/qwen3-0.6b-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jackv\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\rottermaatje-UFzBPA2d-py3.12\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4805, in from_pretrained\n",
      "    is_accelerate_available()\n",
      "  File \"C:\\Users\\jackv\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\rottermaatje-UFzBPA2d-py3.12\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 108, in _is_package_available\n",
      "    return importlib.util.find_spec(pkg_name) is not None\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jackv\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\util.py\", line 108, in find_spec\n",
      "    parent = __import__(parent_name, fromlist=['__path__'])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ModuleNotFoundError: No module named 'accelerate'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 7, in <module>\n",
      "  File \"<stdin>\", line 8, in process_prompts\n",
      "  File \"<stdin>\", line 63, in __init__\n",
      "  File \"<stdin>\", line 103, in _load_model\n",
      "  File \"C:\\Users\\jackv\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\rottermaatje-UFzBPA2d-py3.12\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 604, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jackv\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\rottermaatje-UFzBPA2d-py3.12\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jackv\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\rottermaatje-UFzBPA2d-py3.12\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4806, in from_pretrained\n",
      "    raise ValueError(\n",
      "ValueError: Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` requires `accelerate`. You can install it with `pip install accelerate`\n"
     ]
    }
   ],
   "source": [
    "eval_results = []\n",
    "for i, model in enumerate(small_models):\n",
    "    # Clean up memory before loading the next model\n",
    "    ServeLLM.cleanup_all()\n",
    "    \n",
    "    print(f\"PROCESSING {model}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    eval_results.append(process_prompts(small_models[i], TEST_PROMPTS))\n",
    "\n",
    "# Display results\n",
    "for i, (prompt, response) in enumerate(zip(TEST_PROMPTS, eval_results[i])):\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3b4c5d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 2: Implement a function to evaluate the responses\n",
    "\n",
    "Now that you have the responses, you need to evaluate them. For this simple case, you'll check if the expected keywords are present in the model's output. This is a basic form of evaluation, but it's a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d5e6f",
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate_responses(responses, expected_keywords):\n",
    "    \"\"\"\n",
    "    Evaluate model responses based on expected keywords.\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = []\n",
    "    for i, response in enumerate(responses):\n",
    "        # Simple check: does the response contain the expected keyword?\n",
    "        if expected_keywords[i] in response:\n",
    "            scores.append(1) # Correct\n",
    "        else:\n",
    "            scores.append(0) # Incorrect\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3g4h5i6j",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now, let's use this function to evaluate the results from each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7k8l9m0n",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "evaluation_scores = []\n",
    "for i, model_responses in enumerate(eval_results):\n",
    "    scores = evaluate_responses(model_responses, EXPECTED_KEYWORDS)\n",
    "    evaluation_scores.append(scores)\n",
    "    \n",
    "    print(f\"Scores for {small_models[i]}: {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4p5q6r7s",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 3: Create a summary DataFrame\n",
    "\n",
    "To make the results easier to read, let's put them into a pandas DataFrame. This is a common practice in data science for organizing and analyzing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8t9u0v1w",
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def create_summary_df(models, prompts, results, scores):\n",
    "    \"\"\"\n",
    "    Create a pandas DataFrame to summarize the evaluation results.\n",
    "    \"\"\"\n",
    "    \n",
    "    summary_data = []\n",
    "    for i, model_name in enumerate(models):\n",
    "        for j, prompt in enumerate(prompts):\n",
    "            summary_data.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Prompt\": prompt,\n",
    "                \"Response\": results[i][j],\n",
    "                \"Score\": scores[i][j]\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(summary_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5x6y7z8a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now, create and display the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c1d2e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "summary_df = create_summary_df(small_models, TEST_PROMPTS, eval_results, evaluation_scores)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4g5h6i",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, you've gone through a simple but complete LLM evaluation workflow:\n",
    "1.  **Processed Prompts**: You sent a series of questions to different language models.\n",
    "2.  **Evaluated Responses**: You checked the models' answers for correctness based on keywords.\n",
    "3.  **Summarized Results**: You organized everything into a clean table for easy comparison.\n",
    "\n",
    "This is a foundational approach to model evaluation. In more advanced scenarios, you might use more sophisticated metrics, larger datasets, or even other LLMs to help with the evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
