{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "rottermaatje-header"
            },
            "source": [
                "# ðŸ¤– RotterMaatje Fine-Tuning Notebook\n",
                "\n",
                "This notebook fine-tunes a **Qwen3-4B-Instruct** model on RotterMaatje conversational data using:\n",
                "1. **SFT (Supervised Fine-Tuning)** - Refining the empathetic conversation style\n",
                "2. **DPO (Direct Preference Optimization)** - Aligning with safety and accuracy preferences\n",
                "\n",
                "**Requirements:**\n",
                "- Google Colab with GPU (T4 or better)\n",
                "- Upload your data files when prompted\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section-install"
            },
            "source": [
                "## ðŸ“¦ Step 1: Install Dependencies\n",
                "\n",
                "This will install Unsloth and all required packages. Takes ~2-3 minutes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install-unsloth"
            },
            "outputs": [],
            "source": [
                "%%capture\n",
                "# Install Unsloth (optimized for Colab)\n",
                "!pip install unsloth\n",
                "# Install the latest compatible packages\n",
                "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "verify-gpu"
            },
            "outputs": [],
            "source": [
                "# Verify GPU availability\n",
                "import torch\n",
                "print(f\"ðŸ”§ PyTorch version: {torch.__version__}\")\n",
                "print(f\"ðŸŽ® CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"ðŸ“Š GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"ðŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                "else:\n",
                "    print(\"âš ï¸ No GPU detected! Go to Runtime > Change runtime type > GPU\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section-upload"
            },
            "source": [
                "## ðŸ“¤ Step 2: Upload Your Data\n",
                "\n",
                "You need to upload two files:\n",
                "1. `synthetic_train.jsonl` - SFT training data\n",
                "2. `synthetic_dpo.jsonl` - DPO preference data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "upload-data"
            },
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "import os\n",
                "\n",
                "print(\"ðŸ“ Please upload your training data files...\")\n",
                "print(\"   - synthetic_train.jsonl (SFT data)\")\n",
                "print(\"   - synthetic_dpo.jsonl (DPO data)\")\n",
                "print()\n",
                "\n",
                "uploaded = files.upload()\n",
                "\n",
                "for filename in uploaded.keys():\n",
                "    print(f\"âœ… Uploaded: {filename} ({len(uploaded[filename]) / 1024:.1f} KB)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "verify-data"
            },
            "outputs": [],
            "source": [
                "# Verify data files\n",
                "import json\n",
                "\n",
                "def count_lines(filepath):\n",
                "    with open(filepath, 'r', encoding='utf-8') as f:\n",
                "        return sum(1 for _ in f)\n",
                "\n",
                "if os.path.exists('synthetic_train.jsonl'):\n",
                "    sft_count = count_lines('synthetic_train.jsonl')\n",
                "    print(f\"âœ… SFT data: {sft_count} conversations\")\n",
                "else:\n",
                "    print(\"âŒ Missing: synthetic_train.jsonl\")\n",
                "\n",
                "if os.path.exists('synthetic_dpo.jsonl'):\n",
                "    dpo_count = count_lines('synthetic_dpo.jsonl')\n",
                "    print(f\"âœ… DPO data: {dpo_count} preference pairs\")\n",
                "else:\n",
                "    print(\"âŒ Missing: synthetic_dpo.jsonl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section-sft"
            },
            "source": [
                "## ðŸŽ“ Step 3: Supervised Fine-Tuning (SFT)\n",
                "\n",
                "Refine the instruct model with RotterMaatje's specific persona and knowledge."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "load-model-sft"
            },
            "outputs": [],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "import torch\n",
                "\n",
                "# Configuration\n",
                "MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"  # 4B model - fits on Colab T4\n",
                "MAX_SEQ_LENGTH = 2048\n",
                "\n",
                "print(f\"ðŸš€ Loading {MODEL_NAME}...\")\n",
                "\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name=MODEL_NAME,\n",
                "    max_seq_length=MAX_SEQ_LENGTH,\n",
                "    load_in_4bit=True,\n",
                "    dtype=None,  # Auto-detect\n",
                ")\n",
                "\n",
                "print(\"ðŸŽ¨ Adding LoRA adapters...\")\n",
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r = 16, # Rank\n",
                "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
                "    lora_alpha = 16,\n",
                "    lora_dropout = 0,\n",
                "    bias = \"none\",    \n",
                "    use_gradient_checkpointing = \"unsloth\", \n",
                "    random_state = 3407,\n",
                ")\n",
                "\n",
                "print(\"âœ… Model loaded and adapters attached!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup-chat-template"
            },
            "outputs": [],
            "source": [
                "from unsloth.chat_templates import get_chat_template\n",
                "\n",
                "# Apply chat template for Qwen\n",
                "tokenizer = get_chat_template(\n",
                "    tokenizer,\n",
                "    chat_template=\"qwen-2.5\",  # Compatible with latest Qwen models\n",
                ")\n",
                "\n",
                "print(\"âœ… Chat template configured!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "add-lora-sft"
            },
            "outputs": [],
            "source": [
                "# Add LoRA adapters for efficient fine-tuning\n",
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r=16,  # LoRA rank\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    lora_alpha=16,\n",
                "    lora_dropout=0,\n",
                "    bias=\"none\",\n",
                "    use_gradient_checkpointing=\"unsloth\",\n",
                ")\n",
                "\n",
                "print(\"âœ… LoRA adapters added!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "load-sft-data"
            },
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "# Load SFT dataset\n",
                "dataset = load_dataset(\"json\", data_files=\"synthetic_train.jsonl\", split=\"train\")\n",
                "\n",
                "def formatting_prompts_func(examples):\n",
                "    \"\"\"Format conversations into chat template format.\"\"\"\n",
                "    convos = examples[\"messages\"]\n",
                "    texts = [\n",
                "        tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
                "        for convo in convos\n",
                "    ]\n",
                "    return {\"text\": texts}\n",
                "\n",
                "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
                "\n",
                "print(f\"âœ… Loaded {len(dataset)} training conversations\")\n",
                "print(f\"\\nðŸ“ Sample formatted text:\\n{dataset[0]['text'][:500]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "train-sft"
            },
            "outputs": [],
            "source": [
                "from trl import SFTTrainer, SFTConfig\n",
                "\n",
                "print(\"ðŸŽ¯ Starting SFT Training...\")\n",
                "print(\"   This will take ~10-20 minutes on a T4 GPU\")\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    train_dataset=dataset,\n",
                "    dataset_text_field=\"text\",\n",
                "    max_seq_length=MAX_SEQ_LENGTH,\n",
                "    dataset_num_proc=2,\n",
                "    packing=False,\n",
                "    args=SFTConfig(\n",
                "        per_device_train_batch_size=2,\n",
                "        gradient_accumulation_steps=4,\n",
                "        warmup_steps=5,\n",
                "        max_steps=100,  # Adjust based on dataset size\n",
                "        learning_rate=2e-4,\n",
                "        fp16=not torch.cuda.is_bf16_supported(),\n",
                "        bf16=torch.cuda.is_bf16_supported(),\n",
                "        logging_steps=5,\n",
                "        weight_decay=0.01,\n",
                "        output_dir=\"outputs_sft\",\n",
                "        optim=\"adamw_8bit\",\n",
                "        seed=3407,\n",
                "        report_to=\"none\",\n",
                "    ),\n",
                ")\n",
                "\n",
                "# Train!\n",
                "trainer_stats = trainer.train()\n",
                "\n",
                "print(\"\\nâœ… SFT Training complete!\")\n",
                "print(f\"   Training loss: {trainer_stats.training_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "save-sft-checkpoint"
            },
            "outputs": [],
            "source": [
                "# Save SFT checkpoint for DPO\n",
                "model.save_pretrained(\"lora_sft_checkpoint\")\n",
                "tokenizer.save_pretrained(\"lora_sft_checkpoint\")\n",
                "print(\"âœ… SFT checkpoint saved!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section-dpo"
            },
            "source": [
                "## ðŸŽ¯ Step 4: Direct Preference Optimization (DPO)\n",
                "\n",
                "Align the model with safety and accuracy preferences using chosen/rejected pairs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup-dpo"
            },
            "outputs": [],
            "source": [
                "from unsloth import PatchDPOTrainer\n",
                "from trl import DPOTrainer, DPOConfig\n",
                "from datasets import load_dataset\n",
                "\n",
                "# Patch DPO for Unsloth optimizations\n",
                "PatchDPOTrainer()\n",
                "\n",
                "# Load DPO dataset\n",
                "dpo_dataset = load_dataset(\"json\", data_files=\"synthetic_dpo.jsonl\", split=\"train\")\n",
                "\n",
                "print(f\"âœ… Loaded {len(dpo_dataset)} DPO preference pairs\")\n",
                "print(f\"\\nðŸ“ Sample DPO pair:\")\n",
                "print(f\"   Prompt: {dpo_dataset[0]['prompt'][:100]}...\")\n",
                "print(f\"   Chosen: {dpo_dataset[0]['chosen'][:100]}...\")\n",
                "print(f\"   Rejected: {dpo_dataset[0]['rejected'][:100]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "train-dpo"
            },
            "outputs": [],
            "source": [
                "print(\"ðŸŽ¯ Starting DPO Training...\")\n",
                "print(\"   This will take ~10-15 minutes on a T4 GPU\")\n",
                "\n",
                "dpo_trainer = DPOTrainer(\n",
                "    model=model,\n",
                "    ref_model=None,  # Unsloth handles this for PEFT\n",
                "    train_dataset=dpo_dataset,\n",
                "    tokenizer=tokenizer,\n",
                "    args=DPOConfig(\n",
                "        per_device_train_batch_size=1,\n",
                "        gradient_accumulation_steps=8,\n",
                "        warmup_steps=5,\n",
                "        max_steps=60,\n",
                "        learning_rate=5e-6,  # Lower LR for DPO\n",
                "        fp16=not torch.cuda.is_bf16_supported(),\n",
                "        bf16=torch.cuda.is_bf16_supported(),\n",
                "        logging_steps=5,\n",
                "        optim=\"adamw_8bit\",\n",
                "        weight_decay=0.05,\n",
                "        output_dir=\"outputs_dpo\",\n",
                "        seed=3407,\n",
                "        report_to=\"none\",\n",
                "        beta=0.1,  # DPO KL penalty\n",
                "        max_length=1024,\n",
                "        max_prompt_length=512,\n",
                "    ),\n",
                ")\n",
                "\n",
                "# Train!\n",
                "dpo_stats = dpo_trainer.train()\n",
                "\n",
                "print(\"\\nâœ… DPO Training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section-export"
            },
            "source": [
                "## ðŸ’¾ Step 5: Export Model\n",
                "\n",
                "Export the fine-tuned model in GGUF format for use with llama.cpp or Ollama."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "export-gguf"
            },
            "outputs": [],
            "source": [
                "print(\"ðŸ“¦ Exporting model to GGUF format...\")\n",
                "print(\"   This may take 5-10 minutes\")\n",
                "\n",
                "# Export as Q4_K_M quantization (good balance of size/quality)\n",
                "model.save_pretrained_gguf(\n",
                "    \"rottermaatje_q4_k_m\",\n",
                "    tokenizer,\n",
                "    quantization_method=\"q4_k_m\"\n",
                ")\n",
                "\n",
                "print(\"\\nâœ… Model exported successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "list-outputs"
            },
            "outputs": [],
            "source": [
                "# List exported files\n",
                "import os\n",
                "\n",
                "print(\"ðŸ“ Exported files:\")\n",
                "for f in os.listdir(\"rottermaatje_q4_k_m\"):\n",
                "    size = os.path.getsize(f\"rottermaatje_q4_k_m/{f}\") / 1e6\n",
                "    print(f\"   {f} ({size:.1f} MB)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "download-model"
            },
            "outputs": [],
            "source": [
                "# Download the GGUF file\n",
                "from google.colab import files\n",
                "import glob\n",
                "\n",
                "gguf_files = glob.glob(\"rottermaatje_q4_k_m/*.gguf\")\n",
                "if gguf_files:\n",
                "    print(f\"â¬‡ï¸ Downloading: {gguf_files[0]}\")\n",
                "    files.download(gguf_files[0])\n",
                "else:\n",
                "    print(\"âŒ No GGUF file found!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section-test"
            },
            "source": [
                "## ðŸ§ª Step 6: Test the Model (Optional)\n",
                "\n",
                "Quick test to verify the fine-tuned model works correctly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "test-model"
            },
            "outputs": [],
            "source": [
                "# Enable inference mode\n",
                "FastLanguageModel.for_inference(model)\n",
                "\n",
                "# Test prompts in different languages\n",
                "test_prompts = [\n",
                "    \"Ik ben dakloos en heb honger. Waar kan ik eten krijgen?\",\n",
                "    \"I see a homeless person on the street. What can I do to help?\",\n",
                "    \"Ø£ÙŠÙ† ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø£ÙˆÙ‰ Ù„ÙŠÙ„ÙŠ ÙÙŠ Ø±ÙˆØªØ±Ø¯Ø§Ù…ØŸ\",\n",
                "    \"Gdzie mogÄ™ znaleÅºÄ‡ nocleg w Rotterdamie?\"\n",
                "]\n",
                "\n",
                "print(\"ðŸ§ª Testing fine-tuned model:\\n\")\n",
                "\n",
                "for prompt in test_prompts:\n",
                "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
                "    inputs = tokenizer.apply_chat_template(\n",
                "        messages,\n",
                "        tokenize=True,\n",
                "        add_generation_prompt=True,\n",
                "        return_tensors=\"pt\"\n",
                "    ).to(\"cuda\")\n",
                "\n",
                "    outputs = model.generate(\n",
                "        input_ids=inputs,\n",
                "        max_new_tokens=256,\n",
                "        temperature=0.7,\n",
                "        top_p=0.9,\n",
                "        do_sample=True,\n",
                "    )\n",
                "\n",
                "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    # Extract just the assistant response\n",
                "    if \"assistant\" in response.lower():\n",
                "        response = response.split(\"assistant\")[-1].strip()\n",
                "\n",
                "    print(f\"ðŸ“ User: {prompt[:60]}...\")\n",
                "    print(f\"ðŸ¤– RotterMaatje: {response[:200]}...\")\n",
                "    print(\"-\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section-done"
            },
            "source": [
                "## âœ… Done!\n",
                "\n",
                "Your fine-tuned RotterMaatje model has been:\n",
                "1. âœ… Trained with SFT on empathetic conversations\n",
                "2. âœ… Aligned with DPO on safety preferences\n",
                "3. âœ… Exported as GGUF for local deployment\n",
                "\n",
                "### Next Steps:\n",
                "1. Download the `.gguf` file from the outputs\n",
                "2. Place it in your `Jack/models/` directory\n",
                "3. Configure Ollama or llama.cpp to use the model\n",
                "\n",
                "### Ollama Setup:\n",
                "```bash\n",
                "# Create a Modelfile\n",
                "echo 'FROM ./rottermaatje_q4_k_m.gguf' > Modelfile\n",
                "\n",
                "# Create the model in Ollama\n",
                "ollama create rottermaatje -f Modelfile\n",
                "\n",
                "# Test it\n",
                "ollama run rottermaatje\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": [],
            "toc_visible": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
